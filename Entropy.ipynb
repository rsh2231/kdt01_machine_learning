{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "211fa276",
   "metadata": {},
   "source": [
    "# 🔐 엔트로피(Entropy)란?\n",
    "\n",
    "## ✅ 한 줄 정의\n",
    "> **엔트로피는 '정보의 무질서도' 또는 '불확실성의 정도'를 나타내는 값**입니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 📦 일상적인 예시로 이해하기\n",
    "\n",
    "### 🎲 예시 1: 주사위 던지기\n",
    "- **공정한 주사위**를 던지면 1~6이 **똑같은 확률로 나올 수 있어요.**\n",
    "- 어떤 숫자가 나올지 **정말 알기 어렵죠**?  \n",
    "→ 이럴 때 **엔트로피가 높다**고 말해요.  \n",
    "→ 정보가 **많이 들어 있음**!\n",
    "\n",
    "### 🍫 예시 2: 초콜릿 박스\n",
    "- 초콜릿 박스를 열었는데, **항상 똑같은 맛**만 나온다면?\n",
    "→ 결과가 **예측 가능**해서 놀라울 게 없죠.  \n",
    "→ 이럴 땐 **엔트로피가 낮아요.**\n",
    "\n",
    "---\n",
    "\n",
    "## 🎯 요점 정리\n",
    "\n",
    "| 상황 | 엔트로피 |\n",
    "|------|-----------|\n",
    "| 결과가 예측 불가능함 (랜덤함이 큼) | 🔥 높음 |\n",
    "| 결과가 거의 정해져 있음 (예측 쉬움) | ❄️ 낮음 |\n",
    "\n",
    "---\n",
    "\n",
    "## 📊 정보 이론에서의 의미\n",
    "\n",
    "- **엔트로피는 정보량을 수치로 나타낸 것**입니다.\n",
    "- **불확실성이 높을수록 → 정보량(엔트로피)도 많아집니다.**\n",
    "- 일반적으로 다음과 같이 계산합니다:\n",
    "\n",
    "\\[\n",
    "H = -\\sum_{i=1}^{n} p_i \\log_2 p_i\n",
    "\\]\n",
    "\n",
    "- \\(p_i\\): i번째 사건의 확률  \n",
    "- 로그를 사용하는 이유: **희귀한 사건일수록 정보량이 크기 때문**\n",
    "\n",
    "---\n",
    "\n",
    "## 🧠 왜 중요할까?\n",
    "\n",
    "- **머신러닝**에서는 데이터를 분류할 때,  \n",
    "  가장 **정보를 많이 주는 특성**을 찾기 위해 엔트로피를 사용해요.\n",
    "- 예: 의사결정나무(Decision Tree)는  \n",
    "  **엔트로피가 낮아지는 방향으로** 데이터를 나눕니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 📝 요약\n",
    "\n",
    "> 엔트로피는 \"얼마나 예측하기 어려운가?\"를 숫자로 표현한 개념이에요.  \n",
    "> - 무질서하거나 랜덤할수록 높고  \n",
    "> - 규칙적이고 예측 가능할수록 낮습니다.  \n",
    "\n",
    "---\n",
    "\n",
    "## 💬 비유로 다시 말해보면...\n",
    "\n",
    "> \"엔트로피는 놀람의 정도!\"  \n",
    "> - **매번 똑같은 결과** = 놀람 없음 = 엔트로피 낮음  \n",
    "> - **예상 못 한 결과** = 놀람 큼 = 엔트로피 높음\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df17678b",
   "metadata": {},
   "source": [
    "# 🧠 머신러닝에서 엔트로피는 왜 중요한가?\n",
    "\n",
    "## 👉 의사결정나무(Decision Tree)란?\n",
    "\n",
    "- 데이터를 분류하거나 예측할 때 쓰는 **나무 구조 모델**입니다.\n",
    "- **질문을 반복하면서** 데이터를 **점점 더 명확하게 나눠가는 방식**이에요.\n",
    "\n",
    "예:\n",
    "\n",
    "▶️ 날씨가 맑은가?\n",
    "\n",
    "├─ 예 → 야구함\n",
    "\n",
    "└─ 아니오 → ▶️ 습도가 높은가?\n",
    "\n",
    "├─ 예 → 야구 안 함\n",
    "\n",
    "└─ 아니오 → 야구함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c6b88b",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "## 📦 엔트로피의 역할\n",
    "\n",
    "### 🎯 목표: 가장 \"정보가 많은 질문\"부터 하기\n",
    "- 각 질문(특성)은 데이터를 나누는데 도움을 줄 수도, 아닐 수도 있어요.\n",
    "- **엔트로피는 \"얼마나 불확실한가\"를 수치로 알려주기 때문에**,  \n",
    "  → 의사결정나무는 **엔트로피가 가장 많이 줄어드는 특성**부터 사용해요.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔍 예시: 날씨로 야구할지 말지 결정\n",
    "\n",
    "| 날씨   | 온도 | 야구함? |\n",
    "|--------|------|---------|\n",
    "| 맑음   | 덥다 | 아니오  |\n",
    "| 맑음   | 보통 | 예      |\n",
    "| 흐림   | 보통 | 예      |\n",
    "| 비     | 시원 | 아니오  |\n",
    "\n",
    "### ⚙️ 엔트로피 계산 방식\n",
    "\n",
    "\\[\n",
    "H(D) = -p_{yes} \\log_2 p_{yes} - p_{no} \\log_2 p_{no}\n",
    "\\]\n",
    "\n",
    "- 전체 데이터에 대해 먼저 **현재의 엔트로피**를 구하고,\n",
    "- 각 특성(날씨, 온도 등)으로 나눴을 때 **새로운 엔트로피**를 구해요.\n",
    "- **엔트로피가 많이 줄어든다 → 좋은 특성!**\n",
    "\n",
    "이걸 우리는 **정보 이득(Information Gain)**이라고 부릅니다.\n",
    "\n",
    "---\n",
    "\n",
    "## 🔁 반복적으로 나무를 쌓음\n",
    "\n",
    "1. 데이터 전체의 엔트로피 계산\n",
    "2. 각 특성별로 데이터를 나눈 후, 엔트로피 감소량(정보 이득) 계산\n",
    "3. **가장 정보 이득이 큰 특성**을 기준으로 나눔\n",
    "4. 자식 노드에서도 1~3을 반복!\n",
    "\n",
    "---\n",
    "\n",
    "## 📌 요약\n",
    "\n",
    "| 개념 | 설명 |\n",
    "|------|------|\n",
    "| 엔트로피 | 현재 데이터의 불확실성 (무질서도) |\n",
    "| 정보 이득 | 엔트로피가 얼마나 줄어드는가 |\n",
    "| 목표 | 정보 이득이 가장 큰 특성부터 나누기 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 직관적인 정리\n",
    "\n",
    "> 의사결정나무는 데이터를 분류할 때  \n",
    "> **\"가장 혼란스러운 걸 가장 빨리 줄이는 질문\"부터 시작**합니다.  \n",
    "> 이때 그 혼란스러움을 재는 도구가 바로 **엔트로피**입니다!\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
